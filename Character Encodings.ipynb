{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc47e741",
   "metadata": {},
   "source": [
    "# Character Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a79258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful character encoding module\n",
    "# Required for this environment: $ mamba install -n base -c conda-forge chardet\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2932d",
   "metadata": {},
   "source": [
    "### What are encodings?\n",
    "**Character encodings** are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). There are many different encodings, and if you tried to read in text with a different encoding than the one it was originally written in, you ended up with scrambled text called \"mojibake\" (said like mo-gee-bah-kay). Here's an example of mojibake:\n",
    "\n",
    "æ–‡å—åŒ–ã??\n",
    "\n",
    "You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and they look like this:\n",
    "\n",
    "����������\n",
    "\n",
    "Character encoding mismatches are less common today than they used to be, but it's definitely still a problem. There are lots of different character encodings, but the main one you need to know is UTF-8.\n",
    "\n",
    "UTF-8 is **the** standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble.\n",
    "\n",
    "It was pretty hard to deal with encodings in Python 2, but thankfully in Python 3 it's a lot simpler. (Kaggle Notebooks only use Python 3.) There are two main data types you'll encounter when working with text in Python 3. One is is the string, which is what text is by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87874e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# check to see what datatype it is\n",
    "type(before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8041684",
   "metadata": {},
   "source": [
    "The other data is the bytes data type, which is a sequence of integers. You can convert a string into bytes by specifying which encoding it's in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c0a08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# check the type\n",
    "type(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250d767",
   "metadata": {},
   "source": [
    "If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. (ASCII is an older character encoding that doesn't really work for writing any language other than English.) Here you can see that our euro symbol has been replaced with some mojibake that looks like \"\\xe2\\x82\\xac\" when it's printed as if it were an ASCII string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb55af19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'This is the euro symbol: \\xe2\\x82\\xac'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at what the bytes look like\n",
    "after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f31f2",
   "metadata": {},
   "source": [
    "When we convert our bytes back to a string with the correct encoding, we can see that our text is all there correctly, which is great! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b92fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the euro symbol: €\n"
     ]
    }
   ],
   "source": [
    "# convert it back to utf-8\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72738c9c",
   "metadata": {},
   "source": [
    "However, when we try to use a different encoding to map our bytes into a string, we get an error. This is because the encoding we're trying to use doesn't know what to do with the bytes we're trying to pass it. You need to tell Python the encoding that the byte string is actually supposed to be in.\n",
    "\n",
    ">You can think of different encodings as different ways of recording music. You can record the same music on a CD, cassette tape or 8-track. While the music may sound more-or-less the same, you need to use the right equipment to play the music from each recording format. The correct decoder is like a cassette player or a CD player. If you try to play a cassette in a CD player, it just won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbf6764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'UnicodeDecodeError'>\n"
     ]
    }
   ],
   "source": [
    "# try to decode our bytes with the ascii encoding\n",
    "try:\n",
    "    print(after.decode(\"ascii\"))\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe19b3",
   "metadata": {},
   "source": [
    "We can also run into trouble if we try to use the wrong encoding to map from a string to bytes. Like I said earlier, strings are UTF-8 by default in Python 3, so if we try to treat them like they were in another encoding we'll create problems.\n",
    "\n",
    "For example, if we try to convert a string to bytes for ASCII using encode(), we can ask for the bytes to be what they would be if the text was in ASCII. Since our text isn't in ASCII, though, there will be some characters it can't handle. We can automatically replace the characters that ASCII can't handle. If we do that, however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0636073",
   "metadata": {},
   "source": [
    "#### Reading in files with encoding problems\n",
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0bcd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the euro symbol: ?\n"
     ]
    }
   ],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"ascii\", errors = \"replace\")\n",
    "\n",
    "# convert it back to utf-8\n",
    "print(after.decode(\"ascii\"))\n",
    "\n",
    "# We've lost the original underlying byte string! It's been \n",
    "# replaced with the underlying byte string for the unknown character :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db310257",
   "metadata": {},
   "source": [
    "This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding. The best time to convert non UTF-8 input into UTF-8 is when you read in files, which we'll talk about next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b042c5a",
   "metadata": {},
   "source": [
    "### Reading in files with encoding problems\n",
    "Most files you'll encounter will probably be encoded with UTF-8. This is what Python expects by default, so most of the time you won't run into problems. However, sometimes you'll get an error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2815a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'UnicodeDecodeError'>\n"
     ]
    }
   ],
   "source": [
    "# try to read in a file not in UTF-8\n",
    "try:\n",
    "    kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed1f90",
   "metadata": {},
   "source": [
    "Notice that we get the same UnicodeDecodeError we got when we tried to decode UTF-8 bytes as if they were ASCII! This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. A better way, though, is to use the chardet module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess.\n",
    "\n",
    "I'm going to just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. (Especially with a large file this can be very slow.) Another reason to just look at the first part of the file is that we can see by looking at the error message that the first problem is the 11th character. So we probably only need to look at the first little bit of the file to figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14fba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201612.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7467d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05969bb9",
   "metadata": {},
   "source": [
    "So chardet is 73% confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f8a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdk01/anaconda3/envs/test/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09 11:36:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26 00:20:50</td>\n",
       "      <td>45000</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16 04:24:11</td>\n",
       "      <td>5000</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29 01:00:00</td>\n",
       "      <td>19500</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01 13:38:27</td>\n",
       "      <td>50000</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>52375</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>52375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               name   \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000004038                                     Where is Hank?   \n",
       "2  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "3  1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "        category  main_category  currency             deadline   goal   \\\n",
       "0          Poetry     Publishing       GBP  2015-10-09 11:36:00   1000   \n",
       "1  Narrative Film   Film & Video       USD  2013-02-26 00:20:50  45000   \n",
       "2           Music          Music       USD  2012-04-16 04:24:11   5000   \n",
       "3    Film & Video   Film & Video       USD  2015-08-29 01:00:00  19500   \n",
       "4     Restaurants           Food       USD  2016-04-01 13:38:27  50000   \n",
       "\n",
       "             launched  pledged       state  backers  country  usd pledged   \\\n",
       "0  2015-08-11 12:12:28        0      failed        0       GB            0   \n",
       "1  2013-01-12 00:20:50      220      failed        3       US          220   \n",
       "2  2012-03-17 03:24:11        1      failed        1       US            1   \n",
       "3  2015-07-04 08:35:03     1283    canceled       14       US         1283   \n",
       "4  2016-02-26 13:38:27    52375  successful      224       US        52375   \n",
       "\n",
       "  Unnamed: 13 Unnamed: 14 Unnamed: 15  Unnamed: 16  \n",
       "0         NaN         NaN         NaN          NaN  \n",
       "1         NaN         NaN         NaN          NaN  \n",
       "2         NaN         NaN         NaN          NaN  \n",
       "3         NaN         NaN         NaN          NaN  \n",
       "4         NaN         NaN         NaN          NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n",
    "\n",
    "# look at the first few lines\n",
    "kickstarter_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f41351",
   "metadata": {},
   "source": [
    "Yep, looks like chardet was right! The file reads in with no problem (although we do get a warning about datatypes) and when we look at the first few rows it seems to be fine.\n",
    "\n",
    "What if the encoding chardet guesses isn't right? Since chardet is basically just a fancy guesser, sometimes it will guess the wrong encoding. One thing you can try is looking at more or less of the file and seeing if you get a different result and then try that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3563e",
   "metadata": {},
   "source": [
    "#### Saving your files with UTF-8 encoding\n",
    "Finally, once you've gone through all the trouble of getting your file into UTF-8, you'll probably want to keep it that way. The easiest way to do that is to save your files with UTF-8 encoding. The good news is, since UTF-8 is the standard encoding in Python, when you save a file it will be saved as UTF-8 by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3e86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "\n",
    "# kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104d028",
   "metadata": {},
   "source": [
    "# Exercise: Character Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc55c44",
   "metadata": {},
   "source": [
    "### Get our environment set up\n",
    "\n",
    "The first thing we'll need to do is load in the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d505fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful character encoding module\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100ce4d",
   "metadata": {},
   "source": [
    "### 1) What are encodings?\n",
    "\n",
    "You're working with a dataset composed of bytes.  Run the code cell below to print a sample entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d85aa3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xa7A\\xa6n'\n",
      "data type: <class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "sample_entry = b'\\xa7A\\xa6n'\n",
    "print(sample_entry)\n",
    "print('data type:', type(sample_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd0b33",
   "metadata": {},
   "source": [
    "You notice that it doesn't use the standard UTF-8 encoding. \n",
    "\n",
    "Use the next code cell to create a variable `new_entry` that changes the encoding from `\"big5-tw\"` to `\"utf-8\"`.  `new_entry` should have the bytes datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c594a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_entry = sample_entry.decode('big5-tw').encode('utf-8')\n",
    "new_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac76d2d",
   "metadata": {},
   "source": [
    "### 2) Reading in files with encoding problems\n",
    "\n",
    "Use the code cell below to read in this file at path `\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\"`.  \n",
    "\n",
    "Figure out what the correct encoding should be and read in the file to a DataFrame `police_killings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d75b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Estimate the encoding with chardet:\n",
    "# When the file was read without encoding below, it failed at the 28000 ch. The 1st 10000 where all ascii.\n",
    "\n",
    "with open('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', 'rb') as rawdata:\n",
    "    result1 = chardet.detect(rawdata.read(30000))\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561cd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load in the DataFrame correctly.\n",
    "# Use Windows-1252 as determined above,\n",
    "police_killings = pd.read_csv(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", encoding='Windows-1252')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c6297",
   "metadata": {},
   "source": [
    "### 3) Saving your files with UTF-8 encoding¶\n",
    "Save a version of the police killings dataset to CSV with UTF-8 encoding. Your answer will be marked correct after saving this file.\n",
    "\n",
    "Note: When using the to_csv() method, supply only the name of the file (e.g., \"my_file.csv\"). This saves the file at the filepath \"/kaggle/working/my_file.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24ebcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "police_killings.to_csv('my_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef73d72",
   "metadata": {},
   "source": [
    "### (Optional) More practice\n",
    "\n",
    "Check out [this dataset of files in different character encodings](https://www.kaggle.com/rtatman/character-encoding-examples). Can you read in all the files with their original encodings and them save them out as UTF-8 files?\n",
    "\n",
    "If you have a file that's in UTF-8 but has just a couple of weird-looking characters in it, you can try out the [ftfy module](https://ftfy.readthedocs.io/en/latest/#) and see if it helps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7cae733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qCoding(filename, quantity=-1):\n",
    "    with open(filename, 'rb') as f:\n",
    "        raw = f.read()\n",
    "        v = chardet.detect(raw)['encoding']\n",
    "        return raw, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d87039e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Language</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die_ISO-8859-1.txt</td>\n",
       "      <td>Die Fürstin</td>\n",
       "      <td>Kasimir Edschmid</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>German</td>\n",
       "      <td>13314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harpers_ASCII.txt</td>\n",
       "      <td>Harper's Round Table, October 8, 1895</td>\n",
       "      <td>Various</td>\n",
       "      <td>ASCII</td>\n",
       "      <td>English</td>\n",
       "      <td>29094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>olaf_Windows-1251.txt</td>\n",
       "      <td>Olaf van Geldern</td>\n",
       "      <td>Pencho Slaveykov</td>\n",
       "      <td>Windows 1251</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>2790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>portugal_ISO-8859-1.txt</td>\n",
       "      <td>Portugal enfermo por vicios, e abusos de ambos...</td>\n",
       "      <td>José Daniel Rodrigues da Costa</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>14215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shisei_UTF-8.txt</td>\n",
       "      <td>Shisei</td>\n",
       "      <td>Junichiro Tanizaki</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>4809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yan_BIG-5.txt</td>\n",
       "      <td>Yan shi jia xun</td>\n",
       "      <td>Yan Zhitui</td>\n",
       "      <td>BIG-5</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      File                                               Text  \\\n",
       "0       die_ISO-8859-1.txt                                        Die Fürstin   \n",
       "1        harpers_ASCII.txt              Harper's Round Table, October 8, 1895   \n",
       "2    olaf_Windows-1251.txt                                   Olaf van Geldern   \n",
       "3  portugal_ISO-8859-1.txt  Portugal enfermo por vicios, e abusos de ambos...   \n",
       "4         shisei_UTF-8.txt                                             Shisei   \n",
       "5            yan_BIG-5.txt                                    Yan shi jia xun   \n",
       "\n",
       "                           Author      Encoding    Language  Words  \n",
       "0                Kasimir Edschmid    ISO-8859-1      German  13314  \n",
       "1                         Various         ASCII     English  29094  \n",
       "2                Pencho Slaveykov  Windows 1251   Bulgarian   2790  \n",
       "3  José Daniel Rodrigues da Costa    ISO-8859-1  Portuguese  14215  \n",
       "4              Junichiro Tanizaki         UTF-8    Japanese   4809  \n",
       "5                      Yan Zhitui         BIG-5     Chinese   2538  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = Path.home() / 'notebooks' / 'input' / 'character_encoding_examples'\n",
    "catalog = pd.read_csv(directory / 'file_guide.csv')\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23edf112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Because ascii is subset of UTF8, a file of pure ascii in UTF8 is reported as ASCII\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_774/496107427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Note: Because ascii is subset of UTF8, a file of pure ascii in UTF8 is reported as ASCII'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# pd.concat([catalog, pd.DataFrame(result).transpose()], axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9184\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9186\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   9187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9188\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 107\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_specification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mcross_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_validate_specification\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0mcommon_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m                     raise MergeError(\n\u001b[0m\u001b[1;32m   1343\u001b[0m                         \u001b[0;34m\"No common columns to perform merge on. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                         \u001b[0;34mf\"Merge options: left_on={self.left_on}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMergeError\u001b[0m: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False"
     ]
    }
   ],
   "source": [
    "# adaptation of https://www.kaggle.com/keithmurray/data-cleaning-character-encoding-practice\n",
    "# where all the files are worked on iteratively\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from chardet import detect\n",
    "\n",
    "# Make a catalog of the files to examine\n",
    "directory = Path.home() / 'notebooks' / 'input' / 'character_encoding_examples'\n",
    "catalog = pd.read_csv(directory / 'file_guide.csv')\n",
    "\n",
    "result = {}\n",
    "\n",
    "for i, fn in enumerate(catalog.File):\n",
    "    \n",
    "    # Construct next file to process\n",
    "    filename = directory / fn\n",
    "\n",
    "    # Estimate its encoding. Retain raw data for decoding.\n",
    "    with open(filename, 'rb') as f:\n",
    "        raw = f.read()\n",
    "        estimated_encoding = detect(raw)['encoding']\n",
    "\n",
    "    # Encode using the estimated encoding and check the result\n",
    "    new_encoding = detect(raw.decode(estimated_encoding).encode())['encoding']\n",
    "    \n",
    "    \n",
    "    result[fn] = {'Estimated Encoding': estimated_encoding, 'New Encoding': new_encoding}\n",
    "        \n",
    "print('Note: Because ascii is subset of UTF8, a file of pure ascii in UTF8 is reported as ASCII')\n",
    "# pd.concat([catalog, pd.DataFrame(result).transpose()], axis=0)\n",
    "catalog.merge(pd.DataFrame(result).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01148cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimated Encoding</th>\n",
       "      <th>New Encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>die_ISO-8859-1.txt</th>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harpers_ASCII.txt</th>\n",
       "      <td>ascii</td>\n",
       "      <td>ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olaf_Windows-1251.txt</th>\n",
       "      <td>windows-1251</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portugal_ISO-8859-1.txt</th>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shisei_UTF-8.txt</th>\n",
       "      <td>UTF-8-SIG</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yan_BIG-5.txt</th>\n",
       "      <td>Big5</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Estimated Encoding New Encoding\n",
       "die_ISO-8859-1.txt              ISO-8859-1        utf-8\n",
       "harpers_ASCII.txt                    ascii        ascii\n",
       "olaf_Windows-1251.txt         windows-1251        utf-8\n",
       "portugal_ISO-8859-1.txt         ISO-8859-1        utf-8\n",
       "shisei_UTF-8.txt                 UTF-8-SIG        utf-8\n",
       "yan_BIG-5.txt                         Big5        utf-8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0f27aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Language</th>\n",
       "      <th>Words</th>\n",
       "      <th>Estimated Encoding</th>\n",
       "      <th>New Encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die_ISO-8859-1.txt</td>\n",
       "      <td>Die Fürstin</td>\n",
       "      <td>Kasimir Edschmid</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>German</td>\n",
       "      <td>13314</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harpers_ASCII.txt</td>\n",
       "      <td>Harper's Round Table, October 8, 1895</td>\n",
       "      <td>Various</td>\n",
       "      <td>ASCII</td>\n",
       "      <td>English</td>\n",
       "      <td>29094</td>\n",
       "      <td>ascii</td>\n",
       "      <td>ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>olaf_Windows-1251.txt</td>\n",
       "      <td>Olaf van Geldern</td>\n",
       "      <td>Pencho Slaveykov</td>\n",
       "      <td>Windows 1251</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>2790</td>\n",
       "      <td>windows-1251</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>portugal_ISO-8859-1.txt</td>\n",
       "      <td>Portugal enfermo por vicios, e abusos de ambos...</td>\n",
       "      <td>José Daniel Rodrigues da Costa</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>14215</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shisei_UTF-8.txt</td>\n",
       "      <td>Shisei</td>\n",
       "      <td>Junichiro Tanizaki</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>4809</td>\n",
       "      <td>UTF-8-SIG</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yan_BIG-5.txt</td>\n",
       "      <td>Yan shi jia xun</td>\n",
       "      <td>Yan Zhitui</td>\n",
       "      <td>BIG-5</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2538</td>\n",
       "      <td>Big5</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      File                                               Text  \\\n",
       "0       die_ISO-8859-1.txt                                        Die Fürstin   \n",
       "1        harpers_ASCII.txt              Harper's Round Table, October 8, 1895   \n",
       "2    olaf_Windows-1251.txt                                   Olaf van Geldern   \n",
       "3  portugal_ISO-8859-1.txt  Portugal enfermo por vicios, e abusos de ambos...   \n",
       "4         shisei_UTF-8.txt                                             Shisei   \n",
       "5            yan_BIG-5.txt                                    Yan shi jia xun   \n",
       "\n",
       "                           Author      Encoding    Language  Words  \\\n",
       "0                Kasimir Edschmid    ISO-8859-1      German  13314   \n",
       "1                         Various         ASCII     English  29094   \n",
       "2                Pencho Slaveykov  Windows 1251   Bulgarian   2790   \n",
       "3  José Daniel Rodrigues da Costa    ISO-8859-1  Portuguese  14215   \n",
       "4              Junichiro Tanizaki         UTF-8    Japanese   4809   \n",
       "5                      Yan Zhitui         BIG-5     Chinese   2538   \n",
       "\n",
       "  Estimated Encoding New Encoding  \n",
       "0         ISO-8859-1        utf-8  \n",
       "1              ascii        ascii  \n",
       "2       windows-1251        utf-8  \n",
       "3         ISO-8859-1        utf-8  \n",
       "4          UTF-8-SIG        utf-8  \n",
       "5               Big5        utf-8  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaptation of https://www.kaggle.com/keithmurray/data-cleaning-character-encoding-practice\n",
    "# where all the files are worked on iteratively\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from chardet import detect\n",
    "\n",
    "# Make a catalog of the files to examine\n",
    "directory = Path.home() / 'notebooks' / 'input' / 'character_encoding_examples'\n",
    "catalog = pd.read_csv(directory / 'file_guide.csv')\n",
    "\n",
    "\n",
    "def encoding(x, sample=10000, directory=directory):\n",
    "    # Find the encoding of the original file and the encoding if it is converted to UTF8\n",
    "    \n",
    "    filename = directory / x\n",
    "\n",
    "    # Find original encoding from a sample. Save contents for decode-encode\n",
    "    with open(filename, 'rb') as f:\n",
    "        raw = f.read()\n",
    "        estimated_encoding = detect(raw[:sample])['encoding']\n",
    "\n",
    "    # Find the encoding after decoding as estimated encoding and then after re-encoding as UTF8\n",
    "    new_encoding = detect(raw.decode(estimated_encoding).encode()[:sample])['encoding']\n",
    "    \n",
    "    # Return as a Series so it can be added as new columns in the catalog\n",
    "    return pd.Series([estimated_encoding, new_encoding])\n",
    "\n",
    "print('Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII')\n",
    "catalog[['Estimated Encoding', 'New Encoding']] = catalog['File'].apply(encoding)\n",
    "catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb537442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Language</th>\n",
       "      <th>Words</th>\n",
       "      <th>Estimated Encoding</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>New Encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die_ISO-8859-1.txt</td>\n",
       "      <td>Die Fürstin</td>\n",
       "      <td>Kasimir Edschmid</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>German</td>\n",
       "      <td>13314</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harpers_ASCII.txt</td>\n",
       "      <td>Harper's Round Table, October 8, 1895</td>\n",
       "      <td>Various</td>\n",
       "      <td>ASCII</td>\n",
       "      <td>English</td>\n",
       "      <td>29094</td>\n",
       "      <td>ascii</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>olaf_Windows-1251.txt</td>\n",
       "      <td>Olaf van Geldern</td>\n",
       "      <td>Pencho Slaveykov</td>\n",
       "      <td>Windows 1251</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>2790</td>\n",
       "      <td>windows-1251</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>portugal_ISO-8859-1.txt</td>\n",
       "      <td>Portugal enfermo por vicios, e abusos de ambos...</td>\n",
       "      <td>José Daniel Rodrigues da Costa</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>14215</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shisei_UTF-8.txt</td>\n",
       "      <td>Shisei</td>\n",
       "      <td>Junichiro Tanizaki</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>4809</td>\n",
       "      <td>UTF-8-SIG</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yan_BIG-5.txt</td>\n",
       "      <td>Yan shi jia xun</td>\n",
       "      <td>Yan Zhitui</td>\n",
       "      <td>BIG-5</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2538</td>\n",
       "      <td>Big5</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      File                                               Text  \\\n",
       "0       die_ISO-8859-1.txt                                        Die Fürstin   \n",
       "1        harpers_ASCII.txt              Harper's Round Table, October 8, 1895   \n",
       "2    olaf_Windows-1251.txt                                   Olaf van Geldern   \n",
       "3  portugal_ISO-8859-1.txt  Portugal enfermo por vicios, e abusos de ambos...   \n",
       "4         shisei_UTF-8.txt                                             Shisei   \n",
       "5            yan_BIG-5.txt                                    Yan shi jia xun   \n",
       "\n",
       "                           Author      Encoding    Language  Words  \\\n",
       "0                Kasimir Edschmid    ISO-8859-1      German  13314   \n",
       "1                         Various         ASCII     English  29094   \n",
       "2                Pencho Slaveykov  Windows 1251   Bulgarian   2790   \n",
       "3  José Daniel Rodrigues da Costa    ISO-8859-1  Portuguese  14215   \n",
       "4              Junichiro Tanizaki         UTF-8    Japanese   4809   \n",
       "5                      Yan Zhitui         BIG-5     Chinese   2538   \n",
       "\n",
       "  Estimated Encoding  Confidence New Encoding  \n",
       "0         ISO-8859-1    0.671095        utf-8  \n",
       "1              ascii    1.000000        ascii  \n",
       "2       windows-1251    0.990000        utf-8  \n",
       "3         ISO-8859-1    0.730000        utf-8  \n",
       "4          UTF-8-SIG    1.000000        utf-8  \n",
       "5               Big5    0.990000        utf-8  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adaptation of https://www.kaggle.com/keithmurray/data-cleaning-character-encoding-practice\n",
    "# to use pandas apply\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from chardet import detect\n",
    "\n",
    "# Make a catalog of the files to examine\n",
    "directory = Path.home() / 'notebooks' / 'input' / 'character_encoding_examples'\n",
    "catalog = pd.read_csv(directory / 'file_guide.csv')\n",
    "\n",
    "\n",
    "def find_encoding(x, sample=10001, directory=directory):\n",
    "    # Find the encoding of the original file and the encoding if it is converted to UTF8\n",
    "    \n",
    "    filename = directory / x\n",
    "\n",
    "    # Find original encoding from a sample. Save contents for decode-encode\n",
    "    with open(filename, 'rb') as f:\n",
    "        raw = f.read(sample)\n",
    "        try:\n",
    "            estimate = detect(raw)\n",
    "        except Exception as e:\n",
    "            print(f'Estimate: {e.__class__} in {filename}')\n",
    "            return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "    # Find the encoding after decoding as estimated encoding and then after re-encoding as UTF8\n",
    "    try:\n",
    "        new = detect(raw.decode(estimate['encoding']).encode())\n",
    "    except Exception as e:\n",
    "        print(f'New: {e.__class__} in {filename}')\n",
    "        return pd.Series([estimate['encoding'], estimate['confidence'], np.nan])\n",
    "       \n",
    "    # Return as a Series so it can be added as new columns in the catalog\n",
    "    return pd.Series([estimate['encoding'], estimate['confidence'], new['encoding']])\n",
    "\n",
    "\n",
    "print('Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII')\n",
    "\n",
    "catalog[['Estimated Encoding', 'Confidence', 'New Encoding']] = catalog['File'].apply(find_encoding)\n",
    "catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9eecf81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'estimate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_774/4159379931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mcatalog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Estimated Encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'New Encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'File'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4354\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \"\"\"\n\u001b[0;32m-> 4356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4358\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1090\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1093\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_774/4159379931.py\u001b[0m in \u001b[0;36mencoding\u001b[0;34m(x, sample, directory)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Find the encoding after decoding as estimated encoding and then after re-encoding as UTF8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mnew_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Return as a Series so it can be added as new columns in the catalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'estimate' is not defined"
     ]
    }
   ],
   "source": [
    "# Adaptation of https://www.kaggle.com/keithmurray/data-cleaning-character-encoding-practice\n",
    "# to use pandas apply\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from chardet import detect\n",
    "\n",
    "# Make a catalog of the files to examine\n",
    "directory = Path.home() / 'notebooks' / 'input' / 'character_encoding_examples'\n",
    "catalog = pd.read_csv(directory / 'file_guide.csv')\n",
    "\n",
    "\n",
    "def encoding(x, sample=10001, directory=directory):\n",
    "    # Find the encoding of the original file and the encoding if it is converted to UTF8\n",
    "    \n",
    "    filename = directory / x\n",
    "\n",
    "    # Find original encoding from a sample. Save contents for decode-encode\n",
    "    with open(filename, 'rb') as f:\n",
    "        raw = f.read(sample)\n",
    "        estimated_encoding = detect(raw)['encoding']\n",
    "\n",
    "    # Find the encoding after decoding as estimated encoding and then after re-encoding as UTF8\n",
    "    new_encoding = detect(raw.decode(estimated_encoding).encode())\n",
    "    \n",
    "    # Return as a Series so it can be added as new columns in the catalog\n",
    "    return pd.Series([estimated_encoding, new_encoding])\n",
    "\n",
    "\n",
    "print('Explanation: Because ascii is subset of UTF8, a utf8 file of pure ascii is reported as ASCII')\n",
    "\n",
    "catalog[['Estimated Encoding', 'New Encoding']] = catalog['File'].apply(encoding)\n",
    "catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56ccde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "826b8a6c",
   "metadata": {},
   "source": [
    "# Extra SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4a33af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/character_encoding_examples/die_ISO-8859-1.txt\n",
      "../input/character_encoding_examples/file_guide.csv\n",
      "../input/character_encoding_examples/harpers_ASCII.txt\n",
      "../input/character_encoding_examples/olaf_Windows-1251.txt\n",
      "../input/character_encoding_examples/output.die_ISO-8859-1.txt\n",
      "../input/character_encoding_examples/output.harpers_ASCII.txt\n",
      "../input/character_encoding_examples/output.olaf_Windows-1251.txt\n",
      "../input/character_encoding_examples/output.portugal_ISO-8859-1.txt\n",
      "../input/character_encoding_examples/output.shisei_UTF-8.txt\n",
      "../input/character_encoding_examples/output.yan_BIG-5.txt\n",
      "../input/character_encoding_examples/portugal_ISO-8859-1.txt\n",
      "../input/character_encoding_examples/shisei_UTF-8.txt\n",
      "../input/character_encoding_examples/yan_BIG-5.txt\n",
      "../input/earthquake-database/database.csv\n",
      "../input/earthquake-database/database.csv.zip\n",
      "../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\n",
      "../input/kickstarter-projects/ks-projects-201612.csv\n",
      "../input/kickstarter-projects/ks-projects-201612.csv.zip\n",
      "../input/kickstarter-projects/ks-projects-201801.csv\n",
      "../input/kickstarter-projects/ks-projects-201801.csv.zip\n",
      "../input/landslide-events/catalog.csv\n",
      "../input/volcanic-eruptions/database.csv\n",
      "../input/world-happiness-report/world-happiness-report-2021.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# for dirname, _, filenames in os.walk('../input/character_encoding_examples'):\n",
    "for dirname, _, filenames in os.walk('../input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f153c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83329a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'a':1, 'b':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c21ea25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "b 2\n"
     ]
    }
   ],
   "source": [
    "for key,v in dict.items():\n",
    "    print(key, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88e4d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 1)\n",
      "('b', 2)\n"
     ]
    }
   ],
   "source": [
    "for key in dict.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e80c5754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for key in dict.keys():\n",
    "    print(key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef5b9f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for v in dict.values():\n",
    "    print(v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff3622b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413ba593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ->  1\n",
      "1 ->  2\n",
      "2 ->  3\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(lst):\n",
    "    print(i, '-> ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6bfc3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ->  1\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(lst):\n",
    "    if i==1: break\n",
    "    print(i, '-> ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c61c82ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ->  1\n",
      "2 ->  3\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(lst):\n",
    "    if i==1: continue\n",
    "    print(i, '-> ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6172e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    return 'x+2=', x+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a58b3f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('x+2=', 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc2c09a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([dict['a'],dict['b']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51364d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['a'],dict['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d364ca3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1\n",
       "b    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series({k:dict[k] for k in ['a', 'b'] if k in dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ca4a090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2]+[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
